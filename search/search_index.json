{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Elephas: Distributed Deep Learning with Keras & Spark Elephas is an extension of Keras , which allows you to run distributed deep learning models at scale with Spark . Elephas currently supports a number of applications, including: Data-parallel training of deep learning models Distributed hyper-parameter optimization Distributed training of ensemble models Schematically, elephas works as follows. Table of content: * Elephas: Distributed Deep Learning with Keras & Spark * Introduction * Getting started * Basic Spark integration * Spark MLlib integration * Spark ML integration * Distributed hyper-parameter optimization * Distributed training of ensemble models * Discussion * Literature Introduction Elephas brings deep learning with Keras to Spark . Elephas intends to keep the simplicity and high usability of Keras, thereby allowing for fast prototyping of distributed models, which can be run on massive data sets. For an introductory example, see the following iPython notebook . \u1f10\u03bb\u03ad\u03c6\u03b1\u03c2 is Greek for ivory and an accompanying project to \u03ba\u03ad\u03c1\u03b1\u03c2, meaning horn . If this seems weird mentioning, like a bad dream, you should confirm it actually is at the Keras documentation . Elephas also means elephant , as in stuffed yellow elephant. Elephas implements a class of data-parallel algorithms on top of Keras, using Spark's RDDs and data frames. Keras Models are initialized on the driver, then serialized and shipped to workers, alongside with data and broadcasted model parameters. Spark workers deserialize the model, train their chunk of data and send their gradients back to the driver. The \"master\" model on the driver is updated by an optimizer, which takes gradients either synchronously or asynchronously. Getting started Just install elephas from PyPI with, Spark will be installed through pyspark for you. pip install elephas That's it, you should now be able to run Elephas examples. Basic Spark integration After installing both Elephas, you can train a model as follows. First, create a local pyspark context from pyspark import SparkContext, SparkConf conf = SparkConf().setAppName('Elephas_App').setMaster('local[8]') sc = SparkContext(conf=conf) Next, you define and compile a Keras model from keras.models import Sequential from keras.layers.core import Dense, Dropout, Activation from keras.optimizers import SGD model = Sequential() model.add(Dense(128, input_dim=784)) model.add(Activation('relu')) model.add(Dropout(0.2)) model.add(Dense(128)) model.add(Activation('relu')) model.add(Dropout(0.2)) model.add(Dense(10)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer=SGD()) and create an RDD from numpy arrays (or however you want to create an RDD) from elephas.utils.rdd_utils import to_simple_rdd rdd = to_simple_rdd(sc, x_train, y_train) The basic model in Elephas is the SparkModel . You initialize a SparkModel by passing in a compiled Keras model, an update frequency and a parallelization mode. After that you can simply fit the model on your RDD. Elephas fit has the same options as a Keras model, so you can pass epochs , batch_size etc. as you're used to from Keras. from elephas.spark_model import SparkModel spark_model = SparkModel(model, frequency='epoch', mode='asynchronous') spark_model.fit(rdd, epochs=20, batch_size=32, verbose=0, validation_split=0.1) Your script can now be run using spark-submit spark-submit --driver-memory 1G ./your_script.py Increasing the driver memory even further may be necessary, as the set of parameters in a network may be very large and collecting them on the driver eats up a lot of resources. See the examples folder for a few working examples. Spark MLlib integration Following up on the last example, to use Spark's MLlib library with Elephas, you create an RDD of LabeledPoints for supervised training as follows from elephas.utils.rdd_utils import to_labeled_point lp_rdd = to_labeled_point(sc, x_train, y_train, categorical=True) Training a given LabeledPoint-RDD is very similar to what we've seen already from elephas.spark_model import SparkMLlibModel spark_model = SparkMLlibModel(model, frequency='batch', mode='hogwild') spark_model.train(lp_rdd, epochs=20, batch_size=32, verbose=0, validation_split=0.1, categorical=True, nb_classes=nb_classes) Spark ML integration To train a model with a SparkML estimator on a data frame, use the following syntax. df = to_data_frame(sc, x_train, y_train, categorical=True) test_df = to_data_frame(sc, x_test, y_test, categorical=True) estimator = ElephasEstimator(model, epochs=epochs, batch_size=batch_size, frequency='batch', mode='asynchronous', categorical=True, nb_classes=nb_classes) fitted_model = estimator.fit(df) Fitting an estimator results in a SparkML transformer, which we can use for predictions and other evaluations by calling the transform method on it. prediction = fitted_model.transform(test_df) pnl = prediction.select(\"label\", \"prediction\") pnl.show(100) prediction_and_label= pnl.rdd.map(lambda row: (row.label, row.prediction)) metrics = MulticlassMetrics(prediction_and_label) print(metrics.precision()) print(metrics.recall()) Distributed hyper-parameter optimization Hyper-parameter optimization with elephas is based on hyperas , a convenience wrapper for hyperopt and keras. Each Spark worker executes a number of trials, the results get collected and the best model is returned. As the distributed mode in hyperopt (using MongoDB), is somewhat difficult to configure and error prone at the time of writing, we chose to implement parallelization ourselves. Right now, the only available optimization algorithm is random search. The first part of this example is more or less directly taken from the hyperas documentation. We define data and model as functions, hyper-parameter ranges are defined through braces. See the hyperas documentation for more on how this works. from __future__ import print_function from hyperopt import STATUS_OK from hyperas.distributions import choice, uniform def data(): from keras.datasets import mnist from keras.utils import np_utils (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(60000, 784) x_test = x_test.reshape(10000, 784) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 nb_classes = 10 y_train = np_utils.to_categorical(y_train, nb_classes) y_test = np_utils.to_categorical(y_test, nb_classes) return x_train, y_train, x_test, y_test def model(x_train, y_train, x_test, y_test): from keras.models import Sequential from keras.layers.core import Dense, Dropout, Activation from keras.optimizers import RMSprop model = Sequential() model.add(Dense(512, input_shape=(784,))) model.add(Activation('relu')) model.add(Dropout({{uniform(0, 1)}})) model.add(Dense({{choice([256, 512, 1024])}})) model.add(Activation('relu')) model.add(Dropout({{uniform(0, 1)}})) model.add(Dense(10)) model.add(Activation('softmax')) rms = RMSprop() model.compile(loss='categorical_crossentropy', optimizer=rms) model.fit(x_train, y_train, batch_size={{choice([64, 128])}}, nb_epoch=1, show_accuracy=True, verbose=2, validation_data=(x_test, y_test)) score, acc = model.evaluate(x_test, y_test, show_accuracy=True, verbose=0) print('Test accuracy:', acc) return {'loss': -acc, 'status': STATUS_OK, 'model': model.to_yaml()} Once the basic setup is defined, running the minimization is done in just a few lines of code: from elephas.hyperparam import HyperParamModel from pyspark import SparkContext, SparkConf # Create Spark context conf = SparkConf().setAppName('Elephas_Hyperparameter_Optimization').setMaster('local[8]') sc = SparkContext(conf=conf) # Define hyper-parameter model and run optimization hyperparam_model = HyperParamModel(sc) hyperparam_model.minimize(model=model, data=data, max_evals=5) Distributed training of ensemble models Building on the last section, it is possible to train ensemble models with elephas by means of running hyper-parameter optimization on large search spaces and defining a resulting voting classifier on the top-n performing models. With data and model defined as above, this is a simple as running result = hyperparam_model.best_ensemble(nb_ensemble_models=10, model=model, data=data, max_evals=5) In this example an ensemble of 10 models is built, based on optimization of at most 5 runs on each of the Spark workers. Discussion Premature parallelization may not be the root of all evil, but it may not always be the best idea to do so. Keep in mind that more workers mean less data per worker and parallelizing a model is not an excuse for actual learning. So, if you can perfectly well fit your data into memory and you're happy with training speed of the model consider just using keras. One exception to this rule may be that you're already working within the Spark ecosystem and want to leverage what's there. The above SparkML example shows how to use evaluation modules from Spark and maybe you wish to further process the outcome of an elephas model down the road. In this case, we recommend to use elephas as a simple wrapper by setting num_workers=1. Note that right now elephas restricts itself to data-parallel algorithms for two reasons. First, Spark simply makes it very easy to distribute data. Second, neither Spark nor Theano make it particularly easy to split up the actual model in parts, thus making model-parallelism practically impossible to realize. Having said all that, we hope you learn to appreciate elephas as a pretty easy to setup and use playground for data-parallel deep-learning algorithms. Literature [1] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, QV. Le, MZ. Mao, M\u2019A. Ranzato, A. Senior, P. Tucker, K. Yang, and AY. Ng. Large Scale Distributed Deep Networks . [2] F. Niu, B. Recht, C. Re, S.J. Wright HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent [3] C. Noel, S. Osindero. Dogwild! \u2014 Distributed Hogwild for CPU & GPU","title":"Home"},{"location":"#elephas-distributed-deep-learning-with-keras-spark","text":"","title":"Elephas: Distributed Deep Learning with Keras &amp; Spark"},{"location":"#introduction","text":"Elephas brings deep learning with Keras to Spark . Elephas intends to keep the simplicity and high usability of Keras, thereby allowing for fast prototyping of distributed models, which can be run on massive data sets. For an introductory example, see the following iPython notebook . \u1f10\u03bb\u03ad\u03c6\u03b1\u03c2 is Greek for ivory and an accompanying project to \u03ba\u03ad\u03c1\u03b1\u03c2, meaning horn . If this seems weird mentioning, like a bad dream, you should confirm it actually is at the Keras documentation . Elephas also means elephant , as in stuffed yellow elephant. Elephas implements a class of data-parallel algorithms on top of Keras, using Spark's RDDs and data frames. Keras Models are initialized on the driver, then serialized and shipped to workers, alongside with data and broadcasted model parameters. Spark workers deserialize the model, train their chunk of data and send their gradients back to the driver. The \"master\" model on the driver is updated by an optimizer, which takes gradients either synchronously or asynchronously.","title":"Introduction"},{"location":"#getting-started","text":"Just install elephas from PyPI with, Spark will be installed through pyspark for you. pip install elephas That's it, you should now be able to run Elephas examples.","title":"Getting started"},{"location":"#basic-spark-integration","text":"After installing both Elephas, you can train a model as follows. First, create a local pyspark context from pyspark import SparkContext, SparkConf conf = SparkConf().setAppName('Elephas_App').setMaster('local[8]') sc = SparkContext(conf=conf) Next, you define and compile a Keras model from keras.models import Sequential from keras.layers.core import Dense, Dropout, Activation from keras.optimizers import SGD model = Sequential() model.add(Dense(128, input_dim=784)) model.add(Activation('relu')) model.add(Dropout(0.2)) model.add(Dense(128)) model.add(Activation('relu')) model.add(Dropout(0.2)) model.add(Dense(10)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer=SGD()) and create an RDD from numpy arrays (or however you want to create an RDD) from elephas.utils.rdd_utils import to_simple_rdd rdd = to_simple_rdd(sc, x_train, y_train) The basic model in Elephas is the SparkModel . You initialize a SparkModel by passing in a compiled Keras model, an update frequency and a parallelization mode. After that you can simply fit the model on your RDD. Elephas fit has the same options as a Keras model, so you can pass epochs , batch_size etc. as you're used to from Keras. from elephas.spark_model import SparkModel spark_model = SparkModel(model, frequency='epoch', mode='asynchronous') spark_model.fit(rdd, epochs=20, batch_size=32, verbose=0, validation_split=0.1) Your script can now be run using spark-submit spark-submit --driver-memory 1G ./your_script.py Increasing the driver memory even further may be necessary, as the set of parameters in a network may be very large and collecting them on the driver eats up a lot of resources. See the examples folder for a few working examples.","title":"Basic Spark integration"},{"location":"#spark-mllib-integration","text":"Following up on the last example, to use Spark's MLlib library with Elephas, you create an RDD of LabeledPoints for supervised training as follows from elephas.utils.rdd_utils import to_labeled_point lp_rdd = to_labeled_point(sc, x_train, y_train, categorical=True) Training a given LabeledPoint-RDD is very similar to what we've seen already from elephas.spark_model import SparkMLlibModel spark_model = SparkMLlibModel(model, frequency='batch', mode='hogwild') spark_model.train(lp_rdd, epochs=20, batch_size=32, verbose=0, validation_split=0.1, categorical=True, nb_classes=nb_classes)","title":"Spark MLlib integration"},{"location":"#spark-ml-integration","text":"To train a model with a SparkML estimator on a data frame, use the following syntax. df = to_data_frame(sc, x_train, y_train, categorical=True) test_df = to_data_frame(sc, x_test, y_test, categorical=True) estimator = ElephasEstimator(model, epochs=epochs, batch_size=batch_size, frequency='batch', mode='asynchronous', categorical=True, nb_classes=nb_classes) fitted_model = estimator.fit(df) Fitting an estimator results in a SparkML transformer, which we can use for predictions and other evaluations by calling the transform method on it. prediction = fitted_model.transform(test_df) pnl = prediction.select(\"label\", \"prediction\") pnl.show(100) prediction_and_label= pnl.rdd.map(lambda row: (row.label, row.prediction)) metrics = MulticlassMetrics(prediction_and_label) print(metrics.precision()) print(metrics.recall())","title":"Spark ML integration"},{"location":"#distributed-hyper-parameter-optimization","text":"Hyper-parameter optimization with elephas is based on hyperas , a convenience wrapper for hyperopt and keras. Each Spark worker executes a number of trials, the results get collected and the best model is returned. As the distributed mode in hyperopt (using MongoDB), is somewhat difficult to configure and error prone at the time of writing, we chose to implement parallelization ourselves. Right now, the only available optimization algorithm is random search. The first part of this example is more or less directly taken from the hyperas documentation. We define data and model as functions, hyper-parameter ranges are defined through braces. See the hyperas documentation for more on how this works. from __future__ import print_function from hyperopt import STATUS_OK from hyperas.distributions import choice, uniform def data(): from keras.datasets import mnist from keras.utils import np_utils (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(60000, 784) x_test = x_test.reshape(10000, 784) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 nb_classes = 10 y_train = np_utils.to_categorical(y_train, nb_classes) y_test = np_utils.to_categorical(y_test, nb_classes) return x_train, y_train, x_test, y_test def model(x_train, y_train, x_test, y_test): from keras.models import Sequential from keras.layers.core import Dense, Dropout, Activation from keras.optimizers import RMSprop model = Sequential() model.add(Dense(512, input_shape=(784,))) model.add(Activation('relu')) model.add(Dropout({{uniform(0, 1)}})) model.add(Dense({{choice([256, 512, 1024])}})) model.add(Activation('relu')) model.add(Dropout({{uniform(0, 1)}})) model.add(Dense(10)) model.add(Activation('softmax')) rms = RMSprop() model.compile(loss='categorical_crossentropy', optimizer=rms) model.fit(x_train, y_train, batch_size={{choice([64, 128])}}, nb_epoch=1, show_accuracy=True, verbose=2, validation_data=(x_test, y_test)) score, acc = model.evaluate(x_test, y_test, show_accuracy=True, verbose=0) print('Test accuracy:', acc) return {'loss': -acc, 'status': STATUS_OK, 'model': model.to_yaml()} Once the basic setup is defined, running the minimization is done in just a few lines of code: from elephas.hyperparam import HyperParamModel from pyspark import SparkContext, SparkConf # Create Spark context conf = SparkConf().setAppName('Elephas_Hyperparameter_Optimization').setMaster('local[8]') sc = SparkContext(conf=conf) # Define hyper-parameter model and run optimization hyperparam_model = HyperParamModel(sc) hyperparam_model.minimize(model=model, data=data, max_evals=5)","title":"Distributed hyper-parameter optimization"},{"location":"#distributed-training-of-ensemble-models","text":"Building on the last section, it is possible to train ensemble models with elephas by means of running hyper-parameter optimization on large search spaces and defining a resulting voting classifier on the top-n performing models. With data and model defined as above, this is a simple as running result = hyperparam_model.best_ensemble(nb_ensemble_models=10, model=model, data=data, max_evals=5) In this example an ensemble of 10 models is built, based on optimization of at most 5 runs on each of the Spark workers.","title":"Distributed training of ensemble models"},{"location":"#discussion","text":"Premature parallelization may not be the root of all evil, but it may not always be the best idea to do so. Keep in mind that more workers mean less data per worker and parallelizing a model is not an excuse for actual learning. So, if you can perfectly well fit your data into memory and you're happy with training speed of the model consider just using keras. One exception to this rule may be that you're already working within the Spark ecosystem and want to leverage what's there. The above SparkML example shows how to use evaluation modules from Spark and maybe you wish to further process the outcome of an elephas model down the road. In this case, we recommend to use elephas as a simple wrapper by setting num_workers=1. Note that right now elephas restricts itself to data-parallel algorithms for two reasons. First, Spark simply makes it very easy to distribute data. Second, neither Spark nor Theano make it particularly easy to split up the actual model in parts, thus making model-parallelism practically impossible to realize. Having said all that, we hope you learn to appreciate elephas as a pretty easy to setup and use playground for data-parallel deep-learning algorithms.","title":"Discussion"},{"location":"#literature","text":"[1] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, QV. Le, MZ. Mao, M\u2019A. Ranzato, A. Senior, P. Tucker, K. Yang, and AY. Ng. Large Scale Distributed Deep Networks . [2] F. Niu, B. Recht, C. Re, S.J. Wright HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent [3] C. Noel, S. Osindero. Dogwild! \u2014 Distributed Hogwild for CPU & GPU","title":"Literature"},{"location":"why-use-elephas/","text":"Why use Elephas?","title":"Why use Elephas"},{"location":"why-use-elephas/#why-use-elephas","text":"","title":"Why use Elephas?"},{"location":"adapters/spark-ml/","text":"to_data_frame elephas.ml.adapter.to_data_frame(sc, features, labels, categorical=False) Convert numpy arrays of features and labels into Spark DataFrame from_data_frame elephas.ml.adapter.from_data_frame(df, categorical=False, nb_classes=None) Convert DataFrame back to pair of numpy arrays df_to_simple_rdd elephas.ml.adapter.df_to_simple_rdd(df, categorical=False, nb_classes=None, features_col='features', label_col='label') Convert DataFrame into RDD of pairs","title":"Spark ML"},{"location":"adapters/spark-ml/#to_data_frame","text":"elephas.ml.adapter.to_data_frame(sc, features, labels, categorical=False) Convert numpy arrays of features and labels into Spark DataFrame","title":"to_data_frame"},{"location":"adapters/spark-ml/#from_data_frame","text":"elephas.ml.adapter.from_data_frame(df, categorical=False, nb_classes=None) Convert DataFrame back to pair of numpy arrays","title":"from_data_frame"},{"location":"adapters/spark-ml/#df_to_simple_rdd","text":"elephas.ml.adapter.df_to_simple_rdd(df, categorical=False, nb_classes=None, features_col='features', label_col='label') Convert DataFrame into RDD of pairs","title":"df_to_simple_rdd"},{"location":"adapters/spark-mllib/","text":"from_matrix elephas.mllib.adapter.from_matrix(matrix) Convert MLlib Matrix to numpy array to_matrix elephas.mllib.adapter.to_matrix(np_array) Convert numpy array to MLlib Matrix from_vector elephas.mllib.adapter.from_vector(vector) Convert MLlib Vector to numpy array to_vector elephas.mllib.adapter.to_vector(np_array) Convert numpy array to MLlib Vector","title":"Spark mllib"},{"location":"adapters/spark-mllib/#from_matrix","text":"elephas.mllib.adapter.from_matrix(matrix)","title":"from_matrix"},{"location":"adapters/spark-mllib/#convert-mllib-matrix-to-numpy-array","text":"","title":"Convert MLlib Matrix to numpy array"},{"location":"adapters/spark-mllib/#to_matrix","text":"elephas.mllib.adapter.to_matrix(np_array) Convert numpy array to MLlib Matrix","title":"to_matrix"},{"location":"adapters/spark-mllib/#from_vector","text":"elephas.mllib.adapter.from_vector(vector) Convert MLlib Vector to numpy array","title":"from_vector"},{"location":"adapters/spark-mllib/#to_vector","text":"elephas.mllib.adapter.to_vector(np_array) Convert numpy array to MLlib Vector","title":"to_vector"},{"location":"getting-started/faq/","text":"Elephas FAQ: Frequently Asked Questions","title":"FAQ"},{"location":"getting-started/faq/#elephas-faq-frequently-asked-questions","text":"","title":"Elephas FAQ: Frequently Asked Questions"},{"location":"getting-started/getting-started/","text":"Getting started with Elephas","title":"Getting started with Elephas"},{"location":"getting-started/getting-started/#getting-started-with-elephas","text":"","title":"Getting started with Elephas"},{"location":"models/about-elephas-models/","text":"About Elephas models","title":"About Elephas models"},{"location":"models/about-elephas-models/#about-elephas-models","text":"","title":"About Elephas models"},{"location":"models/hyper-param-model/","text":"Distributed hyperparameter search [source] HyperParamModel elephas.hyperparam.HyperParamModel(sc, num_workers=4) HyperParamModel Computes distributed hyper-parameter optimization using Hyperas and Spark.","title":"HyperParamModel"},{"location":"models/hyper-param-model/#distributed-hyperparameter-search","text":"[source]","title":"Distributed hyperparameter search"},{"location":"models/hyper-param-model/#hyperparammodel","text":"elephas.hyperparam.HyperParamModel(sc, num_workers=4) HyperParamModel Computes distributed hyper-parameter optimization using Hyperas and Spark.","title":"HyperParamModel"},{"location":"models/spark-ml-model/","text":"Elephas models for Spark ML [source] ElephasEstimator elephas.ml_model.ElephasEstimator() SparkML Estimator implementation of an elephas model. This estimator takes all relevant arguments for model compilation and training. Returns a trained model in form of a SparkML Model, which is also a Transformer. [source] ElephasTransformer elephas.ml_model.ElephasTransformer() SparkML Transformer implementation. Contains a trained model, with which new feature data can be transformed into labels. load_ml_transformer elephas.ml_model.load_ml_transformer(file_name) load_ml_estimator elephas.ml_model.load_ml_estimator(file_name)","title":"ElephasEstimator"},{"location":"models/spark-ml-model/#elephas-models-for-spark-ml","text":"[source]","title":"Elephas models for Spark ML"},{"location":"models/spark-ml-model/#elephasestimator","text":"elephas.ml_model.ElephasEstimator() SparkML Estimator implementation of an elephas model. This estimator takes all relevant arguments for model compilation and training. Returns a trained model in form of a SparkML Model, which is also a Transformer. [source]","title":"ElephasEstimator"},{"location":"models/spark-ml-model/#elephastransformer","text":"elephas.ml_model.ElephasTransformer() SparkML Transformer implementation. Contains a trained model, with which new feature data can be transformed into labels.","title":"ElephasTransformer"},{"location":"models/spark-ml-model/#load_ml_transformer","text":"elephas.ml_model.load_ml_transformer(file_name)","title":"load_ml_transformer"},{"location":"models/spark-ml-model/#load_ml_estimator","text":"elephas.ml_model.load_ml_estimator(file_name)","title":"load_ml_estimator"},{"location":"models/spark-mllib-model/","text":"SparkModel API [source] SparkMLlibModel elephas.spark_model.SparkMLlibModel(model, mode='asynchronous', frequency='epoch', parameter_server_mode='http', num_workers=4, elephas_optimizer=None, custom_objects=None, batch_size=32) load_spark_model elephas.spark_model.load_spark_model(file_name)","title":"SparkMLlibModel"},{"location":"models/spark-mllib-model/#sparkmodel-api","text":"[source]","title":"SparkModel API"},{"location":"models/spark-mllib-model/#sparkmllibmodel","text":"elephas.spark_model.SparkMLlibModel(model, mode='asynchronous', frequency='epoch', parameter_server_mode='http', num_workers=4, elephas_optimizer=None, custom_objects=None, batch_size=32)","title":"SparkMLlibModel"},{"location":"models/spark-mllib-model/#load_spark_model","text":"elephas.spark_model.load_spark_model(file_name)","title":"load_spark_model"},{"location":"models/spark-model/","text":"SparkModel API [source] SparkModel elephas.spark_model.SparkModel(model, mode='asynchronous', frequency='epoch', parameter_server_mode='http', num_workers=None, elephas_optimizer=None, custom_objects=None, batch_size=32) load_spark_model elephas.spark_model.load_spark_model(file_name)","title":"SparkModel"},{"location":"models/spark-model/#sparkmodel-api","text":"[source]","title":"SparkModel API"},{"location":"models/spark-model/#sparkmodel","text":"elephas.spark_model.SparkModel(model, mode='asynchronous', frequency='epoch', parameter_server_mode='http', num_workers=None, elephas_optimizer=None, custom_objects=None, batch_size=32)","title":"SparkModel"},{"location":"models/spark-model/#load_spark_model","text":"elephas.spark_model.load_spark_model(file_name)","title":"load_spark_model"},{"location":"parameter/client/","text":"Parameter server clients [source] BaseParameterClient elephas.parameter.client.BaseParameterClient() BaseParameterClient Parameter-server clients can do two things: retrieve the current parameters from the corresponding server, and send updates ( delta ) to the server. [source] HttpClient elephas.parameter.client.HttpClient(port=4000) HttpClient Uses HTTP protocol for communication with its corresponding parameter server, namely HttpServer. The HTTP server provides two endpoints, /parameters to get parameters and /update to update the server's parameters.","title":"Clients"},{"location":"parameter/client/#parameter-server-clients","text":"[source]","title":"Parameter server clients"},{"location":"parameter/client/#baseparameterclient","text":"elephas.parameter.client.BaseParameterClient() BaseParameterClient Parameter-server clients can do two things: retrieve the current parameters from the corresponding server, and send updates ( delta ) to the server. [source]","title":"BaseParameterClient"},{"location":"parameter/client/#httpclient","text":"elephas.parameter.client.HttpClient(port=4000) HttpClient Uses HTTP protocol for communication with its corresponding parameter server, namely HttpServer. The HTTP server provides two endpoints, /parameters to get parameters and /update to update the server's parameters.","title":"HttpClient"},{"location":"parameter/server/","text":"Parameter servers [source] BaseParameterServer elephas.parameter.server.BaseParameterServer() BaseParameterServer Parameter servers can be started and stopped. Server implementations have to cater to the needs of their respective BaseParameterClient instances. [source] HttpServer elephas.parameter.server.HttpServer(model, optimizer, mode, port=4000, debug=True, threaded=True, use_reloader=True) HttpServer Flask HTTP server. Defines two routes, /parameters to GET current parameters held by this server, and /update which can be used to POST updates.","title":"Servers"},{"location":"parameter/server/#parameter-servers","text":"[source]","title":"Parameter servers"},{"location":"parameter/server/#baseparameterserver","text":"elephas.parameter.server.BaseParameterServer() BaseParameterServer Parameter servers can be started and stopped. Server implementations have to cater to the needs of their respective BaseParameterClient instances. [source]","title":"BaseParameterServer"},{"location":"parameter/server/#httpserver","text":"elephas.parameter.server.HttpServer(model, optimizer, mode, port=4000, debug=True, threaded=True, use_reloader=True) HttpServer Flask HTTP server. Defines two routes, /parameters to GET current parameters held by this server, and /update which can be used to POST updates.","title":"HttpServer"},{"location":"utils/functional_utils/","text":"add_params elephas.utils.functional_utils.add_params(param_list_left, param_list_right) Add two lists of parameters one by one :param param_list_left: list of numpy arrays :param param_list_right: list of numpy arrays :return: list of numpy arrays subtract_params elephas.utils.functional_utils.subtract_params(param_list_left, param_list_right) Subtract two lists of parameters :param param_list_left: list of numpy arrays :param param_list_right: list of numpy arrays :return: list of numpy arrays get_neutral elephas.utils.functional_utils.get_neutral(array_list) Get list of zero-valued numpy arrays for specified list of numpy arrays :param array_list: list of numpy arrays :return: list of zeros of same shape as input divide_by elephas.utils.functional_utils.divide_by(array_list, num_workers) Divide a list of parameters by an integer num_workers. :param array_list: :param num_workers: :return:","title":"Functional utils"},{"location":"utils/functional_utils/#add_params","text":"elephas.utils.functional_utils.add_params(param_list_left, param_list_right) Add two lists of parameters one by one :param param_list_left: list of numpy arrays :param param_list_right: list of numpy arrays :return: list of numpy arrays","title":"add_params"},{"location":"utils/functional_utils/#subtract_params","text":"elephas.utils.functional_utils.subtract_params(param_list_left, param_list_right) Subtract two lists of parameters :param param_list_left: list of numpy arrays :param param_list_right: list of numpy arrays :return: list of numpy arrays","title":"subtract_params"},{"location":"utils/functional_utils/#get_neutral","text":"elephas.utils.functional_utils.get_neutral(array_list) Get list of zero-valued numpy arrays for specified list of numpy arrays :param array_list: list of numpy arrays :return: list of zeros of same shape as input","title":"get_neutral"},{"location":"utils/functional_utils/#divide_by","text":"elephas.utils.functional_utils.divide_by(array_list, num_workers) Divide a list of parameters by an integer num_workers. :param array_list: :param num_workers: :return:","title":"divide_by"},{"location":"utils/rdd_utils/","text":"to_labeled_point elephas.utils.rdd_utils.to_labeled_point(sc, features, labels, categorical=False) Convert numpy arrays of features and labels into a LabeledPoint RDD for MLlib and ML integration. :param sc: Spark context :param features: numpy array with features :param labels: numpy array with labels :param categorical: boolean, whether labels are already one-hot encoded or not :return: LabeledPoint RDD with features and labels from_labeled_point elephas.utils.rdd_utils.from_labeled_point(rdd, categorical=False, nb_classes=None) Convert a LabeledPoint RDD back to a pair of numpy arrays :param rdd: LabeledPoint RDD :param categorical: boolean, if labels should be one-hot encode when returned :param nb_classes: optional int, indicating the number of class labels :return: pair of numpy arrays, features and labels encode_label elephas.utils.rdd_utils.encode_label(label, nb_classes) One-hot encoding of a single label :param label: class label (int or double without floating point digits) :param nb_classes: int, number of total classes :return: one-hot encoded vector lp_to_simple_rdd elephas.utils.rdd_utils.lp_to_simple_rdd(lp_rdd, categorical=False, nb_classes=None) Convert a LabeledPoint RDD into an RDD of feature-label pairs :param lp_rdd: LabeledPoint RDD of features and labels :param categorical: boolean, if labels should be one-hot encode when returned :param nb_classes: int, number of total classes :return: Spark RDD with feature-label pairs to_simple_rdd elephas.utils.rdd_utils.to_simple_rdd(sc, features, labels) Convert numpy arrays of features and labels into an RDD of pairs. :param sc: Spark context :param features: numpy array with features :param labels: numpy array with labels :return: Spark RDD with feature-label pairs","title":"RDD utils"},{"location":"utils/rdd_utils/#to_labeled_point","text":"elephas.utils.rdd_utils.to_labeled_point(sc, features, labels, categorical=False) Convert numpy arrays of features and labels into a LabeledPoint RDD for MLlib and ML integration. :param sc: Spark context :param features: numpy array with features :param labels: numpy array with labels :param categorical: boolean, whether labels are already one-hot encoded or not :return: LabeledPoint RDD with features and labels","title":"to_labeled_point"},{"location":"utils/rdd_utils/#from_labeled_point","text":"elephas.utils.rdd_utils.from_labeled_point(rdd, categorical=False, nb_classes=None) Convert a LabeledPoint RDD back to a pair of numpy arrays :param rdd: LabeledPoint RDD :param categorical: boolean, if labels should be one-hot encode when returned :param nb_classes: optional int, indicating the number of class labels :return: pair of numpy arrays, features and labels","title":"from_labeled_point"},{"location":"utils/rdd_utils/#encode_label","text":"elephas.utils.rdd_utils.encode_label(label, nb_classes) One-hot encoding of a single label :param label: class label (int or double without floating point digits) :param nb_classes: int, number of total classes :return: one-hot encoded vector","title":"encode_label"},{"location":"utils/rdd_utils/#lp_to_simple_rdd","text":"elephas.utils.rdd_utils.lp_to_simple_rdd(lp_rdd, categorical=False, nb_classes=None) Convert a LabeledPoint RDD into an RDD of feature-label pairs :param lp_rdd: LabeledPoint RDD of features and labels :param categorical: boolean, if labels should be one-hot encode when returned :param nb_classes: int, number of total classes :return: Spark RDD with feature-label pairs","title":"lp_to_simple_rdd"},{"location":"utils/rdd_utils/#to_simple_rdd","text":"elephas.utils.rdd_utils.to_simple_rdd(sc, features, labels) Convert numpy arrays of features and labels into an RDD of pairs. :param sc: Spark context :param features: numpy array with features :param labels: numpy array with labels :return: Spark RDD with feature-label pairs","title":"to_simple_rdd"},{"location":"utils/serialization_utils/","text":"model_to_dict elephas.utils.serialization.model_to_dict(model) Turns a Keras model into a Python dictionary :param model: Keras model instance :return: dictionary with model information dict_to_model elephas.utils.serialization.dict_to_model(dict) Turns a Python dictionary with model architecture and weights back into a Keras model :param dict: dictionary with model and weights keys. :return: Keras model instantiated from dictionary","title":"Serialization utils"},{"location":"utils/serialization_utils/#model_to_dict","text":"elephas.utils.serialization.model_to_dict(model) Turns a Keras model into a Python dictionary :param model: Keras model instance :return: dictionary with model information","title":"model_to_dict"},{"location":"utils/serialization_utils/#dict_to_model","text":"elephas.utils.serialization.dict_to_model(dict) Turns a Python dictionary with model architecture and weights back into a Keras model :param dict: dictionary with model and weights keys. :return: Keras model instantiated from dictionary","title":"dict_to_model"}]}